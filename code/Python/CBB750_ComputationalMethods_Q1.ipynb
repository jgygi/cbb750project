{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CBB750_ComputationalMethods_Q1.ipynb","provenance":[{"file_id":"1Umu9i07otQZ61-s0mXb0oYkqMT4c24NU","timestamp":1619643972752}],"collapsed_sections":[],"authorship_tag":"ABX9TyP9tIJoV/1Czzzt+DF7k7aU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yhfm-7giTuvi","executionInfo":{"status":"ok","timestamp":1619794280370,"user_tz":240,"elapsed":20687,"user":{"displayName":"Jeremy Gygi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiP-IDfMIoYAFBWPRo6_UgnG215WkdTk7rVeAtN=s64","userId":"06612237804500809831"}},"outputId":"ca2eb0d8-0926-452f-a064-95cb267e52a1"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pi9olsY7Vt3c"},"source":["Load the data / packages:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"MgevDg4iBxc2","executionInfo":{"status":"ok","timestamp":1620075575089,"user_tz":240,"elapsed":398,"user":{"displayName":"Jeremy Gygi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiP-IDfMIoYAFBWPRo6_UgnG215WkdTk7rVeAtN=s64","userId":"06612237804500809831"}},"outputId":"9a6bea90-c59f-4744-be5c-a21793f9eaef"},"source":["import sys\n","sys.version"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'3.7.10 (default, Feb 20 2021, 21:17:23) \\n[GCC 7.5.0]'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j7nR9zHgT6Y8","executionInfo":{"status":"ok","timestamp":1619794291814,"user_tz":240,"elapsed":5730,"user":{"displayName":"Jeremy Gygi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiP-IDfMIoYAFBWPRo6_UgnG215WkdTk7rVeAtN=s64","userId":"06612237804500809831"}},"outputId":"8872f42c-dce5-4bee-89a9-e2a1af6057b6"},"source":["# Import pandas (as pd):\n","import pandas as pd\n","# itertools\n","import itertools\n","# Import the Word2Vec and KeyedVectors modules and the nltk library:\n","from gensim.models import Word2Vec, KeyedVectors\n","import nltk\n","nltk.download('punkt')\n","\n","# load the data file with date, deidentified PID, conversation ID:\n","data = pd.read_csv('/content/gdrive/MyDrive/cbb750/project/118106-Messages-class_DEIDENTIFIED.csv', \n","                   encoding = \"ISO-8859-1\",\n","                   na_values = ['--', 'N/A', 'na','NaN'])\n","\n","# convert date type to correct type\n","data['DATE_OF_MESSAGE'] = pd.to_datetime(data['DATE_OF_MESSAGE'])\n","\n","data['PROV_TYPE'] = data['PROV_TYPE'].astype('category') \n","\n","data['ENC_TYPE_DISPLAY'] = data['ENC_TYPE_DISPLAY'].astype('category') \n","\n","data['MYC_MSG_TYP_DISPLAY'] = data['MYC_MSG_TYP_DISPLAY'].astype('category')\n","\n","data['YEAR'] = data['DATE_OF_MESSAGE'].dt.year\n","\n","data = data.dropna()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fIUdUaWQUOx7"},"source":["## Frequencies of Emotional Terms Overtime"]},{"cell_type":"code","metadata":{"id":"yiRBNLShOfCC"},"source":["data_drop_duplicated = data.drop([2307,3328,16192,19534,101,3478,9745,10279,15806,16535,18087,19392,30377,30715,31267,33824])\n","provider = data_drop_duplicated[data_drop_duplicated['TO_PAT_YN']==\"Y\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gjSHdy74Opfa"},"source":["prov_2012 = provider[provider['YEAR']==2012]\n","prov_2013 = provider[provider['YEAR']==2013]\n","prov_2014 = provider[provider['YEAR']==2014]\n","prov_2015 = provider[provider['YEAR']==2015]\n","prov_2016 = provider[provider['YEAR']==2016]\n","prov_2017 = provider[provider['YEAR']==2017]\n","prov_2018 = provider[provider['YEAR']==2018]\n","prov_2019 = provider[provider['YEAR']==2019]\n","prov_2020 = provider[provider['YEAR']==2020]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q6lpieaGQVha"},"source":["df_12 = prov_2012['MESSAGE_CL'].values\n","df_13 = prov_2013['MESSAGE_CL'].values\n","df_14 = prov_2014['MESSAGE_CL'].values\n","df_15 = prov_2015['MESSAGE_CL'].values\n","df_16 = prov_2016['MESSAGE_CL'].values\n","df_17 = prov_2017['MESSAGE_CL'].values\n","df_18 = prov_2018['MESSAGE_CL'].values\n","df_19 = prov_2019['MESSAGE_CL'].values\n","df_20 = prov_2020['MESSAGE_CL'].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u6AgG76lWtZB"},"source":["token_12=[nltk.word_tokenize(row) for row in df_12]\n","token_13=[nltk.word_tokenize(row) for row in df_13]\n","token_14=[nltk.word_tokenize(row) for row in df_14]\n","token_15=[nltk.word_tokenize(row) for row in df_15]\n","token_16=[nltk.word_tokenize(row) for row in df_16]\n","token_17=[nltk.word_tokenize(row) for row in df_17]\n","token_18=[nltk.word_tokenize(row) for row in df_18]\n","token_19=[nltk.word_tokenize(row) for row in df_19]\n","token_20=[nltk.word_tokenize(row) for row in df_20]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cfgt5dm-AT2N"},"source":["def make_lower(token):\n","  for i in range(len(token)):\n","    token[i] = [t.lower() for t in token[i]]\n","  return(token)\n","\n","final_12 = make_lower(token_12)\n","final_13 = make_lower(token_13)\n","final_14 = make_lower(token_14)\n","final_15 = make_lower(token_15)\n","final_16 = make_lower(token_16)\n","final_17 = make_lower(token_17)\n","final_18 = make_lower(token_18)\n","final_19 = make_lower(token_19)\n","final_20 = make_lower(token_20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UnvkzRmZpC71"},"source":["### Stemming"]},{"cell_type":"code","metadata":{"id":"TwNIaGwNjLx3"},"source":["def make_stemmed(token):\n","  for i in range(len(token)):\n","    token[i] = [nltk.PorterStemmer().stem(t) for t in token[i]]\n","  return(token)\n","\n","stemmed_12 = make_stemmed(final_12)\n","stemmed_13 = make_stemmed(final_13)\n","stemmed_14 = make_stemmed(final_14)\n","stemmed_15 = make_stemmed(final_15)\n","stemmed_16 = make_stemmed(final_16)\n","stemmed_17 = make_stemmed(final_17)\n","stemmed_18 = make_stemmed(final_18)\n","stemmed_19 = make_stemmed(final_19)\n","stemmed_20 = make_stemmed(final_20)\n","\n","stem_list =[stemmed_12,\n","            stemmed_13,\n","            stemmed_14,\n","            stemmed_15,\n","            stemmed_16,\n","            stemmed_17,\n","            stemmed_18,\n","            stemmed_19,\n","            stemmed_20]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xDUf6RTWp8v5"},"source":["### Lemmatization"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"trYlrZd5jMjj","executionInfo":{"status":"ok","timestamp":1619794349034,"user_tz":240,"elapsed":630,"user":{"displayName":"Jeremy Gygi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiP-IDfMIoYAFBWPRo6_UgnG215WkdTk7rVeAtN=s64","userId":"06612237804500809831"}},"outputId":"4d468261-7d8e-4b95-b4e3-bacc2f704315"},"source":["nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer() "],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ySqF-usnqXCf"},"source":["def make_lemm(token):\n","  for i in range(len(token)):\n","    token[i] = [WordNetLemmatizer().lemmatize(t,pos='v') for t in token[i]]\n","  return(token)\n","\n","lem_12 = make_lemm(stemmed_12)\n","lem_13 = make_lemm(stemmed_13)\n","lem_14 = make_lemm(stemmed_14)\n","lem_15 = make_lemm(stemmed_15)\n","lem_16 = make_lemm(stemmed_16)\n","lem_17 = make_lemm(stemmed_17)\n","lem_18 = make_lemm(stemmed_18)\n","lem_19 = make_lemm(stemmed_19)\n","lem_20 = make_lemm(stemmed_20)\n","\n","lem_list = [lem_12,\n","            lem_13,\n","            lem_14,\n","            lem_15,\n","            lem_16,\n","            lem_17,\n","            lem_18,\n","            lem_19,\n","            lem_20]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Aau3OtU0TjWZ"},"source":["**Question 1:** what is the frequency distribution of the Providers' use of the terms we are interested in over time?"]},{"cell_type":"code","metadata":{"id":"SKWyblqZTaGE"},"source":["# Make a function to get a W2V model and a CBOW model:\n","def get_w2v_models(tokenized_messages, min_count = 3, size = 300, window = 7):\n","  # CBOW:\n","  print(tokenized_messages)\n","  model = Word2Vec(tokenized_messages, min_count = min_count, size = size, window = window)\n","  model.train\n","  # Skip-Gram:\n","  sg_model = Word2Vec(tokenized_messages, min_count = min_count, size = size, window = window)\n","  sg_model.train\n","  return (model, sg_model)\n","\n","models_per_year = []\n","for i in range(len(lem_list)):\n","  lem = stem_list[i]\n","  models_per_year.append(get_w2v_models(lem))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uaK3M4vm90Kb","executionInfo":{"status":"ok","timestamp":1619705704536,"user_tz":240,"elapsed":201,"user":{"displayName":"Jeremy Gygi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiP-IDfMIoYAFBWPRo6_UgnG215WkdTk7rVeAtN=s64","userId":"06612237804500809831"}},"outputId":"f599a893-658a-4f8d-bd19-2da5d95efab4"},"source":["print(models_per_year[0][0].wv.vocab)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'mr': <gensim.models.keyedvectors.Vocab object at 0x7f0287ed1290>, 'your': <gensim.models.keyedvectors.Vocab object at 0x7f0287ed1450>, 'to': <gensim.models.keyedvectors.Vocab object at 0x7f0287ed1710>, 'you': <gensim.models.keyedvectors.Vocab object at 0x7f0287ed1f50>, 'and': <gensim.models.keyedvectors.Vocab object at 0x7f0287ebfcd0>, 'dr': <gensim.models.keyedvectors.Vocab object at 0x7f0287ebf590>, 'have': <gensim.models.keyedvectors.Vocab object at 0x7f0287ebf710>, 'i': <gensim.models.keyedvectors.Vocab object at 0x7f0287ebc050>, 'she': <gensim.models.keyedvectors.Vocab object at 0x7f0287ebf150>, 'the': <gensim.models.keyedvectors.Vocab object at 0x7f0287ebf050>, 'be': <gensim.models.keyedvectors.Vocab object at 0x7f0287ebc190>, 'not': <gensim.models.keyedvectors.Vocab object at 0x7f0287ec0310>, 'for': <gensim.models.keyedvectors.Vocab object at 0x7f0287ec06d0>, 'at': <gensim.models.keyedvectors.Vocab object at 0x7f0287ec0050>}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VNC4Zn2xWuK5"},"source":["Code to plot the most similar words:"]},{"cell_type":"code","metadata":{"id":"YYf3bGSHWtFW"},"source":["# Adapted from https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n"," \n","import seaborn as sns\n","sns.set_style(\"darkgrid\")\n","\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","\n","def tsnescatterplot(model, word, list_names, model_type, year):\n","    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n","    its list of most similar words, and a list of words.\n","    \"\"\"\n","    arrays = np.empty((0, len(model.wv.__getitem__([word])[0])), dtype='f')\n","    word_labels = [word]\n","    color_list  = ['red']\n","\n","    # adds the vector of the query word\n","    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n","    \n","    # gets list of most similar words\n","    close_words = model.wv.most_similar([word])\n","    \n","    # adds the vector for each of the closest words to the array\n","    #for wrd_score in close_words:\n","    #    wrd_vector = model.wv.__getitem__([wrd_score[0]])\n","    #    word_labels.append(wrd_score[0])\n","    #    color_list.append('blue')\n","    #    arrays = np.append(arrays, wrd_vector, axis=0)\n","    \n","    # adds the vector for each of the words from list_names to the array\n","    for wrd in list_names:\n","        wrd_vector = model.wv.__getitem__([wrd])\n","        word_labels.append(wrd)\n","        color_list.append('green')\n","        arrays = np.append(arrays, wrd_vector, axis=0)\n","    \n","    # Finds t-SNE coordinates for 2 dimensions\n","    np.set_printoptions(suppress=True)\n","    \n","    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(arrays)\n","    \n","    # Sets everything up to plot\n","    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n","                       'y': [y for y in Y[:, 1]],\n","                       'words': word_labels,\n","                       'color': color_list})\n","    \n","    fig, _ = plt.subplots()\n","    fig.set_size_inches(9, 9)\n","    \n","    # Basic plot\n","    p1 = sns.regplot(data=df,\n","                     x=\"x\",\n","                     y=\"y\",\n","                     fit_reg=False,\n","                     marker=\"o\",\n","                     scatter_kws={'s': 40,\n","                                  'facecolors': df['color']\n","                                 }\n","                    )\n","    \n","    # Adds annotations one by one with a loop\n","    for line in range(0, df.shape[0]):\n","         p1.text(df[\"x\"][line],\n","                 df['y'][line],\n","                 '  ' + df[\"words\"][line].title(),\n","                 horizontalalignment='left',\n","                 verticalalignment='bottom', size='medium',\n","                 color=df['color'][line],\n","                 weight='normal'\n","                ).set_size(15)\n","\n","    \n","    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n","    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n","            \n","    plt.title(year + \" | \" + model_type + ' t-SNE visualization for {}'.format(word.title()))\n","\n","    plt.savefig(fname = \"/content/gdrive/MyDrive/cbb750/project/figs/top_20_\" + word + \"_\" + model_type +  \"_\" + year + \".png\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uP5OCHv4iLyP"},"source":["terms = ['happy', 'glad', ':) ', ':(', '!', 'sorry', 'sad', 'delight', 'joy', 'pleased', 'thrilled', 'terrible', 'frustrate', 'good', 'great']\n","terms_modified = ['hello', 'glad', 'sorry', 'sad', 'joy', 'please', ':', ')', 'frustrated', 'good', 'great']\n","terms_modified = [':', ')']\n","terms_modified = ['happi']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"5WPh-Da-gn2I","executionInfo":{"status":"error","timestamp":1619794492161,"user_tz":240,"elapsed":407,"user":{"displayName":"Jeremy Gygi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiP-IDfMIoYAFBWPRo6_UgnG215WkdTk7rVeAtN=s64","userId":"06612237804500809831"}},"outputId":"487b097b-43f0-4fea-91bf-2ac0a9da26b9"},"source":["years = [\"2012\",\n","         \"2013\",\n","         \"2014\",\n","         \"2015\",\n","         \"2016\",\n","         \"2017\",\n","         \"2018\",\n","         \"2019\",\n","         \"2020\"]\n","\n","terms_modified = ['hello', 'glad', 'sorri', 'joy', 'thank', ':', ')', '(', 'frustrat', 'good', 'great', 'enjoy', 'hope']\n","#terms_modified = ['delig']\n","# find another package that finds closest spelled word to 'terms'\n","# sad, upset, cool\n","\n","for term in terms_modified:\n","  for i in range(9):\n","    current_model = models_per_year[i][0]\n","    year = years[i]\n","    try:\n","      count = current_model.wv.vocab[term].count\n","    except:\n","      print(term + \" is not in vocabulary for year \" + year + \"...\")\n","      continue\n","    print((term, count))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-b12c07a46cf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mterms_modified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mcurrent_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels_per_year\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0myear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myears\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'models_per_year' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"xn4YghKyY62B"},"source":["Make plots for the words we are interested in:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3sZsP66uh-DG","executionInfo":{"status":"ok","timestamp":1619664678060,"user_tz":240,"elapsed":86558,"user":{"displayName":"Jeremy Gygi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiP-IDfMIoYAFBWPRo6_UgnG215WkdTk7rVeAtN=s64","userId":"06612237804500809831"}},"outputId":"a8deae5d-b2f6-4306-93da-69a94de14ec7"},"source":["import gensim.downloader\n","glove_vectors = gensim.downloader.load('glove-twitter-25')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[==================================================] 100.0% 104.8/104.8MB downloaded\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lMCAcMrLicSs","executionInfo":{"status":"ok","timestamp":1619664750686,"user_tz":240,"elapsed":243,"user":{"displayName":"Jeremy Gygi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiP-IDfMIoYAFBWPRo6_UgnG215WkdTk7rVeAtN=s64","userId":"06612237804500809831"}},"outputId":"eee31da7-0492-4186-911a-a5602a32c872"},"source":["glove_vectors.most_similar('thrilled')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('pleased', 0.944927453994751),\n"," ('honored', 0.8988169431686401),\n"," ('thoroughly', 0.8970503807067871),\n"," ('delighted', 0.8898038864135742),\n"," ('hugely', 0.8789332509040833),\n"," ('amazed', 0.8786032199859619),\n"," ('honoured', 0.8725692629814148),\n"," ('encouraged', 0.8708942532539368),\n"," ('impressed', 0.8657393455505371),\n"," ('ecstatic', 0.8487899899482727)]"]},"metadata":{"tags":[]},"execution_count":175}]},{"cell_type":"code","metadata":{"id":"aJb7dDzlY6ZZ"},"source":["# Years:\n","years = [\"2012\",\n","         \"2013\",\n","         \"2014\",\n","         \"2015\",\n","         \"2016\",\n","         \"2017\",\n","         \"2018\",\n","         \"2019\",\n","         \"2020\"]\n","\n","for i in range(len(years)):\n","  year = years[i]\n","  models = models_per_year[i]\n","\n","  # CBOW:\n","  for term in terms_modified:\n","    current_model = models[0]\n","    try:\n","      similar_words = [x[0] for x in current_model.wv.most_similar(term, topn = 25)]\n","    except:\n","      print(term + \" is not in vocabulary for year \" + year + \"...\")\n","      continue\n","    model_type = \"CBOW\"\n","    tsnescatterplot(current_model, term, similar_words, model_type, year)\n","\n","  # Skip-Gram:\n","  for term in terms_modified:\n","    current_model = models[1]\n","    try:\n","      similar_words = [x[0] for x in current_model.wv.most_similar(term, topn = 25)]\n","    except:\n","      print(term + \" is not in vocabulary for year \" + year + \"...\")\n","      continue\n","    model_type = \"SG\"\n","    tsnescatterplot(current_model, term, similar_words, model_type, year)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b1B5OxnC0SvQ"},"source":["Frequencies:"]},{"cell_type":"code","metadata":{"id":"lQHwOmCg0UUt"},"source":["years = [\"2012\",\n","         \"2013\",\n","         \"2014\",\n","         \"2015\",\n","         \"2016\",\n","         \"2017\",\n","         \"2018\",\n","         \"2019\",\n","         \"2020\"]\n","\n","terms_total = ['hello', 'glad', 'sorri', 'joy', 'thank', ':', ')', '(', 'frustrat', 'good', 'great', 'enjoy', 'hope', 'total']\n","\n","# CBOW = 0, SG = 1\n","for model_type in range(2):\n","  # Rows = years\n","  # Cols = terms + \"total\"\n","  mat = np.zeros(shape = (len(years), len(terms_total)))\n","  for i, year in enumerate(years):\n","    current_model = models_per_year[i][model_type]\n","    for j, word in enumerate(terms_total):\n","      if word != \"total\":\n","        try:\n","          mat[i, j] = current_model.wv.vocab[word].count\n","        except:\n","          mat[i, j] = 0\n","      else:\n","        total_freq = 0\n","        for word, vocab_obj in current_model.wv.vocab.items():\n","          total_freq = total_freq + current_model.wv.vocab[word].count\n","        mat[i, j] = total_freq\n","\n","  df = pd.DataFrame(mat, index = years, columns = terms_total)\n","  df.to_csv(\"/content/gdrive/MyDrive/cbb750/project/tables/Q1_\" + ['CBOW', 'SG'][model_type] + \".csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aG3fOf2Re5GI","executionInfo":{"status":"ok","timestamp":1619714494836,"user_tz":240,"elapsed":223,"user":{"displayName":"Jeremy Gygi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiP-IDfMIoYAFBWPRo6_UgnG215WkdTk7rVeAtN=s64","userId":"06612237804500809831"}},"outputId":"14b08ece-2b9f-4f9d-af45-01e15fd3824a"},"source":["total_freq = 0\n","for word, vocab_obj in current_model.wv.vocab.items():\n","  total_freq = total_freq + current_model.wv.vocab[word].count\n","\n","total_freq"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["128576"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zVFca9f0WnM5","executionInfo":{"status":"ok","timestamp":1619798189363,"user_tz":240,"elapsed":395,"user":{"displayName":"Jeremy Gygi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiP-IDfMIoYAFBWPRo6_UgnG215WkdTk7rVeAtN=s64","userId":"06612237804500809831"}},"outputId":"dbc0d4b1-3712-4ec1-9795-0680deea35f1"},"source":["l = [1, 2]\n","[l.append(x) for x in [2,3]]\n","print(l)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1, 2, 2, 3]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Uv9nGJCAROUE"},"source":["Get the avg number of \"emotional terms\" per message:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LJ6b8nxTRT7D","executionInfo":{"status":"ok","timestamp":1619798688101,"user_tz":240,"elapsed":1948,"user":{"displayName":"Jeremy Gygi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiP-IDfMIoYAFBWPRo6_UgnG215WkdTk7rVeAtN=s64","userId":"06612237804500809831"}},"outputId":"02e12fd4-814f-4731-943c-73cffd0df9bd"},"source":["totalmeans = {}\n","for i, lem in enumerate(lem_list):\n","  meanvec = []\n","  for message in lem:\n","    m = {}\n","    totalindices = []\n","    for term in terms_modified:\n","      indices = []\n","      for ii, j in enumerate(message):\n","          if j == term:\n","              indices.append(ii)\n","      [totalindices.append(x) for x in indices]\n","      m[term] = len(indices)/len(message)\n","    m[\"total\"] = (len(totalindices) / len(message))\n","    meanvec.append(m)\n","  print(years[i])  \n","  totalmeans[years[i]] = meanvec\n","\n","import json\n","with open('/content/gdrive/MyDrive/cbb750/project/Q1_json.txt', 'w') as outfile:\n","    json.dump(totalmeans, outfile)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2012\n","2013\n","2014\n","2015\n","2016\n","2017\n","2018\n","2019\n","2020\n"],"name":"stdout"}]}]}