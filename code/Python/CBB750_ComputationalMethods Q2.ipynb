{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CBB750_ComputationalMethods.ipynb","provenance":[{"file_id":"1Umu9i07otQZ61-s0mXb0oYkqMT4c24NU","timestamp":1619643972752}],"collapsed_sections":[],"authorship_tag":"ABX9TyMj5K5qq7mWe1l834bwg1ae"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yhfm-7giTuvi","executionInfo":{"status":"ok","timestamp":1619802424974,"user_tz":240,"elapsed":18386,"user":{"displayName":"Jeremy Gygi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiP-IDfMIoYAFBWPRo6_UgnG215WkdTk7rVeAtN=s64","userId":"06612237804500809831"}},"outputId":"5cc09d75-406d-4baf-8c1d-45443f0dd393"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pi9olsY7Vt3c"},"source":["Load the data / packages:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j7nR9zHgT6Y8","executionInfo":{"status":"ok","timestamp":1619802431406,"user_tz":240,"elapsed":5107,"user":{"displayName":"Jeremy Gygi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiP-IDfMIoYAFBWPRo6_UgnG215WkdTk7rVeAtN=s64","userId":"06612237804500809831"}},"outputId":"45053d07-52fc-4dc8-cf13-705e90eda7e6"},"source":["# Import pandas (as pd):\n","import pandas as pd\n","# itertools\n","import itertools\n","# Import the Word2Vec and KeyedVectors modules and the nltk library:\n","from gensim.models import Word2Vec, KeyedVectors\n","import nltk\n","nltk.download('punkt')\n","\n","# load the data file with date, deidentified PID, conversation ID:\n","data = pd.read_csv('/content/gdrive/MyDrive/cbb750/project/118106-Messages-class_DEIDENTIFIED.csv', \n","                   encoding = \"ISO-8859-1\",\n","                   na_values = ['--', 'N/A', 'na','NaN'])\n","\n","# convert date type to correct type\n","data['DATE_OF_MESSAGE'] = pd.to_datetime(data['DATE_OF_MESSAGE'])\n","\n","data['PROV_TYPE'] = data['PROV_TYPE'].astype('category') \n","\n","data['ENC_TYPE_DISPLAY'] = data['ENC_TYPE_DISPLAY'].astype('category') \n","\n","data['MYC_MSG_TYP_DISPLAY'] = data['MYC_MSG_TYP_DISPLAY'].astype('category')\n","\n","data['YEAR'] = data['DATE_OF_MESSAGE'].dt.year\n","\n","data = data.dropna()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fIUdUaWQUOx7"},"source":["## Frequencies of Emotional Terms Overtime"]},{"cell_type":"code","metadata":{"id":"yiRBNLShOfCC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619802434723,"user_tz":240,"elapsed":311,"user":{"displayName":"Jeremy Gygi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiP-IDfMIoYAFBWPRo6_UgnG215WkdTk7rVeAtN=s64","userId":"06612237804500809831"}},"outputId":"4ce6097d-8051-4f9e-9b50-e52d1bc8b209"},"source":["data_drop_duplicated = data.drop([2307,3328,16192,19534,101,3478,9745,10279,15806,16535,18087,19392,30377,30715,31267,33824])\n","provider = data_drop_duplicated[data_drop_duplicated['TO_PAT_YN']==\"Y\"]\n","\n","# Split into Dr. and Resident Nurse:\n","provider_phys = provider[provider['PROV_TYPE'] == \"Physician\"]\n","provider_nurse = provider[provider['PROV_TYPE'] == \"Nurse Practitioner\"]\n","provider_phys.shape\n","provider_nurse.shape"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2495, 14)"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"gjSHdy74Opfa","executionInfo":{"status":"ok","timestamp":1619802437218,"user_tz":240,"elapsed":1208,"user":{"displayName":"Jeremy Gygi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiP-IDfMIoYAFBWPRo6_UgnG215WkdTk7rVeAtN=s64","userId":"06612237804500809831"}}},"source":["def get_lem_list(provider):\n","  prov_2012 = provider[provider['YEAR']==2012]\n","  prov_2013 = provider[provider['YEAR']==2013]\n","  prov_2014 = provider[provider['YEAR']==2014]\n","  prov_2015 = provider[provider['YEAR']==2015]\n","  prov_2016 = provider[provider['YEAR']==2016]\n","  prov_2017 = provider[provider['YEAR']==2017]\n","  prov_2018 = provider[provider['YEAR']==2018]\n","  prov_2019 = provider[provider['YEAR']==2019]\n","  prov_2020 = provider[provider['YEAR']==2020]\n","\n","  df_12 = prov_2012['MESSAGE_CL'].values\n","  df_13 = prov_2013['MESSAGE_CL'].values\n","  df_14 = prov_2014['MESSAGE_CL'].values\n","  df_15 = prov_2015['MESSAGE_CL'].values\n","  df_16 = prov_2016['MESSAGE_CL'].values\n","  df_17 = prov_2017['MESSAGE_CL'].values\n","  df_18 = prov_2018['MESSAGE_CL'].values\n","  df_19 = prov_2019['MESSAGE_CL'].values\n","  df_20 = prov_2020['MESSAGE_CL'].values\n","\n","  token_12=[nltk.word_tokenize(row) for row in df_12]\n","  token_13=[nltk.word_tokenize(row) for row in df_13]\n","  token_14=[nltk.word_tokenize(row) for row in df_14]\n","  token_15=[nltk.word_tokenize(row) for row in df_15]\n","  token_16=[nltk.word_tokenize(row) for row in df_16]\n","  token_17=[nltk.word_tokenize(row) for row in df_17]\n","  token_18=[nltk.word_tokenize(row) for row in df_18]\n","  token_19=[nltk.word_tokenize(row) for row in df_19]\n","  token_20=[nltk.word_tokenize(row) for row in df_20]\n","\n","  def make_lower(token):\n","    for i in range(len(token)):\n","      token[i] = [t.lower() for t in token[i]]\n","    return(token)\n","\n","  final_12 = make_lower(token_12)\n","  final_13 = make_lower(token_13)\n","  final_14 = make_lower(token_14)\n","  final_15 = make_lower(token_15)\n","  final_16 = make_lower(token_16)\n","  final_17 = make_lower(token_17)\n","  final_18 = make_lower(token_18)\n","  final_19 = make_lower(token_19)\n","  final_20 = make_lower(token_20)\n","\n","  def make_stemmed(token):\n","    for i in range(len(token)):\n","      token[i] = [nltk.PorterStemmer().stem(t) for t in token[i]]\n","    return(token)\n","\n","  stemmed_12 = make_stemmed(final_12)\n","  stemmed_13 = make_stemmed(final_13)\n","  stemmed_14 = make_stemmed(final_14)\n","  stemmed_15 = make_stemmed(final_15)\n","  stemmed_16 = make_stemmed(final_16)\n","  stemmed_17 = make_stemmed(final_17)\n","  stemmed_18 = make_stemmed(final_18)\n","  stemmed_19 = make_stemmed(final_19)\n","  stemmed_20 = make_stemmed(final_20)\n","\n","  stem_list =[stemmed_12,\n","              stemmed_13,\n","              stemmed_14,\n","              stemmed_15,\n","              stemmed_16,\n","              stemmed_17,\n","              stemmed_18,\n","              stemmed_19,\n","              stemmed_20]\n","\n","  nltk.download('wordnet')\n","  from nltk.stem import WordNetLemmatizer\n","  lemmatizer = WordNetLemmatizer()\n","\n","  def make_lemm(token):\n","    for i in range(len(token)):\n","      token[i] = [WordNetLemmatizer().lemmatize(t,pos='v') for t in token[i]]\n","    return(token)\n","\n","  lem_12 = make_lemm(stemmed_12)\n","  lem_13 = make_lemm(stemmed_13)\n","  lem_14 = make_lemm(stemmed_14)\n","  lem_15 = make_lemm(stemmed_15)\n","  lem_16 = make_lemm(stemmed_16)\n","  lem_17 = make_lemm(stemmed_17)\n","  lem_18 = make_lemm(stemmed_18)\n","  lem_19 = make_lemm(stemmed_19)\n","  lem_20 = make_lemm(stemmed_20)\n","\n","  lem_list = [lem_12,\n","              lem_13,\n","              lem_14,\n","              lem_15,\n","              lem_16,\n","              lem_17,\n","              lem_18,\n","              lem_19,\n","              lem_20]\n","\n","  return lem_list\n","\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0fHRwMNvrkeC","executionInfo":{"status":"ok","timestamp":1619802460936,"user_tz":240,"elapsed":19892,"user":{"displayName":"Jeremy Gygi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiP-IDfMIoYAFBWPRo6_UgnG215WkdTk7rVeAtN=s64","userId":"06612237804500809831"}},"outputId":"35ae5534-5194-4a75-a352-a0e8ae9a1082"},"source":["prov_lem_list = get_lem_list(provider_phys)\n","nurse_lem_list = get_lem_list(provider_nurse)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Aau3OtU0TjWZ"},"source":["**Question 2:** what is the frequency distribution of the Physician vs. Nurse Practitioner's use of the terms we are interested in over time?"]},{"cell_type":"code","metadata":{"id":"SKWyblqZTaGE","colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1hcPEoDc-x7ASISsNTL9515CuIxm_dR6y"},"executionInfo":{"status":"ok","timestamp":1619735327285,"user_tz":240,"elapsed":17041,"user":{"displayName":"Jeremy Gygi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiP-IDfMIoYAFBWPRo6_UgnG215WkdTk7rVeAtN=s64","userId":"06612237804500809831"}},"outputId":"4a1d21b3-aa03-4757-9026-776b4aa022a0"},"source":["# Make a function to get a W2V model and a CBOW model:\n","def get_w2v_models(tokenized_messages, min_count = 3, size = 300, window = 7):\n","  # CBOW:\n","  print(tokenized_messages)\n","  model = Word2Vec(tokenized_messages, min_count = min_count, size = size, window = window)\n","  model.train\n","  # Skip-Gram:\n","  sg_model = Word2Vec(tokenized_messages, min_count = min_count, size = size, window = window)\n","  sg_model.train\n","  return (model, sg_model)\n","\n","models_per_year_phys = []\n","for i in range(len(prov_lem_list)):\n","  lem = prov_lem_list[i]\n","  models_per_year_phys.append(get_w2v_models(lem))\n","\n","models_per_year_nurse = []\n","for i in range(len(nurse_lem_list)):\n","  lem = nurse_lem_list[i]\n","  if(len(lem)) > 0:\n","    models_per_year_nurse.append(get_w2v_models(lem))\n","  else:\n","    models_per_year_nurse.append(\"None...\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uaK3M4vm90Kb","executionInfo":{"status":"ok","timestamp":1619705704536,"user_tz":240,"elapsed":201,"user":{"displayName":"Jeremy Gygi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiP-IDfMIoYAFBWPRo6_UgnG215WkdTk7rVeAtN=s64","userId":"06612237804500809831"}},"outputId":"f599a893-658a-4f8d-bd19-2da5d95efab4"},"source":["print(models_per_year[0][0].wv.vocab)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'mr': <gensim.models.keyedvectors.Vocab object at 0x7f0287ed1290>, 'your': <gensim.models.keyedvectors.Vocab object at 0x7f0287ed1450>, 'to': <gensim.models.keyedvectors.Vocab object at 0x7f0287ed1710>, 'you': <gensim.models.keyedvectors.Vocab object at 0x7f0287ed1f50>, 'and': <gensim.models.keyedvectors.Vocab object at 0x7f0287ebfcd0>, 'dr': <gensim.models.keyedvectors.Vocab object at 0x7f0287ebf590>, 'have': <gensim.models.keyedvectors.Vocab object at 0x7f0287ebf710>, 'i': <gensim.models.keyedvectors.Vocab object at 0x7f0287ebc050>, 'she': <gensim.models.keyedvectors.Vocab object at 0x7f0287ebf150>, 'the': <gensim.models.keyedvectors.Vocab object at 0x7f0287ebf050>, 'be': <gensim.models.keyedvectors.Vocab object at 0x7f0287ebc190>, 'not': <gensim.models.keyedvectors.Vocab object at 0x7f0287ec0310>, 'for': <gensim.models.keyedvectors.Vocab object at 0x7f0287ec06d0>, 'at': <gensim.models.keyedvectors.Vocab object at 0x7f0287ec0050>}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VNC4Zn2xWuK5"},"source":["Code to plot the most similar words:"]},{"cell_type":"code","metadata":{"id":"YYf3bGSHWtFW"},"source":["# Adapted from https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n"," \n","import seaborn as sns\n","sns.set_style(\"darkgrid\")\n","\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","\n","def tsnescatterplot(model, word, list_names, phys_type, model_type, year):\n","    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n","    its list of most similar words, and a list of words.\n","    \"\"\"\n","    arrays = np.empty((0, len(model.wv.__getitem__([word])[0])), dtype='f')\n","    word_labels = [word]\n","    color_list  = ['red']\n","\n","    # adds the vector of the query word\n","    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n","    \n","    # gets list of most similar words\n","    close_words = model.wv.most_similar([word])\n","    \n","    # adds the vector for each of the closest words to the array\n","    #for wrd_score in close_words:\n","    #    wrd_vector = model.wv.__getitem__([wrd_score[0]])\n","    #    word_labels.append(wrd_score[0])\n","    #    color_list.append('blue')\n","    #    arrays = np.append(arrays, wrd_vector, axis=0)\n","    \n","    # adds the vector for each of the words from list_names to the array\n","    for wrd in list_names:\n","        wrd_vector = model.wv.__getitem__([wrd])\n","        word_labels.append(wrd)\n","        color_list.append('green')\n","        arrays = np.append(arrays, wrd_vector, axis=0)\n","    \n","    # Finds t-SNE coordinates for 2 dimensions\n","    np.set_printoptions(suppress=True)\n","    \n","    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(arrays)\n","    \n","    # Sets everything up to plot\n","    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n","                       'y': [y for y in Y[:, 1]],\n","                       'words': word_labels,\n","                       'color': color_list})\n","    \n","    fig, _ = plt.subplots()\n","    fig.set_size_inches(9, 9)\n","    \n","    # Basic plot\n","    p1 = sns.regplot(data=df,\n","                     x=\"x\",\n","                     y=\"y\",\n","                     fit_reg=False,\n","                     marker=\"o\",\n","                     scatter_kws={'s': 40,\n","                                  'facecolors': df['color']\n","                                 }\n","                    )\n","    \n","    # Adds annotations one by one with a loop\n","    for line in range(0, df.shape[0]):\n","         p1.text(df[\"x\"][line],\n","                 df['y'][line],\n","                 '  ' + df[\"words\"][line].title(),\n","                 horizontalalignment='left',\n","                 verticalalignment='bottom', size='medium',\n","                 color=df['color'][line],\n","                 weight='normal'\n","                ).set_size(15)\n","\n","    \n","    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n","    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n","            \n","    plt.title(year + \" | \" + model_type + ' t-SNE visualization for {}'.format(word.title()))\n","\n","    plt.savefig(fname = \"/content/gdrive/MyDrive/cbb750/project/figs/\" + phys_type + \"_top_20_\" + word + \"_\" + model_type +  \"_\" + year + \".png\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uP5OCHv4iLyP"},"source":["terms = ['happy', 'glad', ':) ', ':(', '!', 'sorry', 'sad', 'delight', 'joy', 'pleased', 'thrilled', 'terrible', 'frustrate', 'good', 'great']\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xn4YghKyY62B"},"source":["Make plots for the words we are interested in:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1-Zox2wiqZ_IuuBGI2H1LkNR7rNEVlLI5"},"id":"aJb7dDzlY6ZZ","executionInfo":{"status":"ok","timestamp":1619735681099,"user_tz":240,"elapsed":172727,"user":{"displayName":"Jeremy Gygi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiP-IDfMIoYAFBWPRo6_UgnG215WkdTk7rVeAtN=s64","userId":"06612237804500809831"}},"outputId":"a46dfc1d-778b-4e3a-e597-7a59c99b7ca0"},"source":["phys_type = \"Physician\"\n","\n","models_per_year = models_per_year_phys\n","\n","# Years:\n","years = [\"2012\",\n","         \"2013\",\n","         \"2014\",\n","         \"2015\",\n","         \"2016\",\n","         \"2017\",\n","         \"2018\",\n","         \"2019\",\n","         \"2020\"]\n","\n","terms_modified = ['hello', 'glad', 'sorri', 'joy', 'thank', ':', ')', '(', 'frustrat', 'good', 'great', 'enjoy', 'hope']\n","\n","for i in range(len(years)):\n","  year = years[i]\n","  models = models_per_year[i]\n","\n","  # CBOW:\n","  for term in terms_modified:\n","    current_model = models[0]\n","    try:\n","      similar_words = [x[0] for x in current_model.wv.most_similar(term, topn = 25)]\n","    except:\n","      print(term + \" is not in vocabulary for year \" + year + \"...\")\n","      continue\n","    model_type = \"CBOW\"\n","    tsnescatterplot(current_model, term, similar_words, phys_type, model_type, year)\n","\n","  # Skip-Gram:\n","  for term in terms_modified:\n","    current_model = models[1]\n","    try:\n","      similar_words = [x[0] for x in current_model.wv.most_similar(term, topn = 25)]\n","    except:\n","      print(term + \" is not in vocabulary for year \" + year + \"...\")\n","      continue\n","    model_type = \"SG\"\n","    tsnescatterplot(current_model, term, similar_words, phys_type, model_type, year)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1OR8XGv4rDyGeGMbPAypUBYnnOR1Qftpk"},"id":"iu-5HuPiwehm","executionInfo":{"status":"ok","timestamp":1619735929775,"user_tz":240,"elapsed":160827,"user":{"displayName":"Jeremy Gygi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiP-IDfMIoYAFBWPRo6_UgnG215WkdTk7rVeAtN=s64","userId":"06612237804500809831"}},"outputId":"de14254e-d2fa-4dbf-b16d-5deff1697272"},"source":["phys_type = \"Nurse\"\n","\n","models_per_year = models_per_year_nurse\n","\n","# Years:\n","years = [\"2012\",\n","         \"2013\",\n","         \"2014\",\n","         \"2015\",\n","         \"2016\",\n","         \"2017\",\n","         \"2018\",\n","         \"2019\",\n","         \"2020\"]\n","\n","terms_total = ['hello', 'glad', 'sorri', 'joy', 'thank', ':', ')', '(', 'frustrat', 'good', 'great', 'enjoy', 'hope']\n","\n","for i in range(len(years)):\n","  year = years[i]\n","  models = models_per_year[i]\n","\n","  # CBOW:\n","  for term in terms_modified:\n","    current_model = models[0]\n","    try:\n","      similar_words = [x[0] for x in current_model.wv.most_similar(term, topn = 25)]\n","    except:\n","      print(term + \" is not in vocabulary for year \" + year + \"...\")\n","      continue\n","    model_type = \"CBOW\"\n","    tsnescatterplot(current_model, term, similar_words, phys_type, model_type, year)\n","\n","  # Skip-Gram:\n","  for term in terms_modified:\n","    current_model = models[1]\n","    try:\n","      similar_words = [x[0] for x in current_model.wv.most_similar(term, topn = 25)]\n","    except:\n","      print(term + \" is not in vocabulary for year \" + year + \"...\")\n","      continue\n","    model_type = \"SG\"\n","    tsnescatterplot(current_model, term, similar_words, phys_type, model_type, year)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"b1B5OxnC0SvQ"},"source":["Frequencies:"]},{"cell_type":"code","metadata":{"id":"lQHwOmCg0UUt"},"source":["years = [\"2012\",\n","         \"2013\",\n","         \"2014\",\n","         \"2015\",\n","         \"2016\",\n","         \"2017\",\n","         \"2018\",\n","         \"2019\",\n","         \"2020\"]\n","\n","terms_total = ['hello', 'glad', 'sorri', 'joy', 'thank', ':', ')', '(', 'frustrat', 'good', 'great', 'enjoy', 'hope', 'total']\n","\n","# CBOW = 0, SG = 1\n","for model_type in range(2):\n","  # Rows = years\n","  # Cols = terms + \"total\"\n","  mat = np.zeros(shape = (len(years), len(terms_total)))\n","  models_per_year = [models_per_year_phys, models_per_year_nurse][model_type]\n","  for i, year in enumerate(years):\n","    current_model = models_per_year[i][0]\n","    for j, word in enumerate(terms_total):\n","      if(isinstance(current_model, str)):\n","        mat[i, j] = 0\n","      else:\n","        if word != \"total\":\n","          try:\n","            mat[i, j] = current_model.wv.vocab[word].count\n","          except:\n","            mat[i, j] = 0\n","        else:\n","          total_freq = 0\n","          for word, vocab_obj in current_model.wv.vocab.items():\n","            total_freq = total_freq + current_model.wv.vocab[word].count\n","          mat[i, j] = total_freq\n","\n","  df = pd.DataFrame(mat, index = years, columns = terms_total)\n","  df.to_csv(\"/content/gdrive/MyDrive/cbb750/project/tables/Q2_\" + ['Physician', 'Nurse'][model_type] + \".csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aG3fOf2Re5GI","executionInfo":{"status":"ok","timestamp":1619802730399,"user_tz":240,"elapsed":2096,"user":{"displayName":"Jeremy Gygi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiP-IDfMIoYAFBWPRo6_UgnG215WkdTk7rVeAtN=s64","userId":"06612237804500809831"}},"outputId":"25c339cd-b6a5-4f47-fb84-c2a3c5a733c1"},"source":["years = [\"2012\",\n","         \"2013\",\n","         \"2014\",\n","         \"2015\",\n","         \"2016\",\n","         \"2017\",\n","         \"2018\",\n","         \"2019\",\n","         \"2020\"]\n","\n","terms_modified = ['hello', 'glad', 'sorri', 'joy', 'thank', ':', ')', '(', 'frustrat', 'good', 'great', 'enjoy', 'hope']\n","\n","for model_type in range(2):\n","  totalmeans = {}\n","  if model_type == 0:\n","    lem_list = prov_lem_list\n","  else:\n","    lem_list = nurse_lem_list\n","  for i, lem in enumerate(lem_list):\n","    meanvec = []\n","    for message in lem:\n","      m = {}\n","      totalindices = []\n","      for term in terms_modified:\n","        indices = []\n","        for ii, j in enumerate(message):\n","            if j == term:\n","                indices.append(ii)\n","        [totalindices.append(x) for x in indices]\n","        m[term] = len(indices)/len(message)\n","      m[\"total\"] = (len(totalindices) / len(message))\n","      meanvec.append(m)\n","    print(years[i])  \n","    totalmeans[years[i]] = meanvec\n","\n","  import json\n","  with open('/content/gdrive/MyDrive/cbb750/project/Q2_' + ['Physician', 'Nurse'][model_type] + '_json.txt', 'w') as outfile:\n","      json.dump(totalmeans, outfile)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["2012\n","2013\n","2014\n","2015\n","2016\n","2017\n","2018\n","2019\n","2020\n","2012\n","2013\n","2014\n","2015\n","2016\n","2017\n","2018\n","2019\n","2020\n"],"name":"stdout"}]}]}